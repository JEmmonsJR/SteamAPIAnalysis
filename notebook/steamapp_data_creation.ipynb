{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Steam Application portion of the Steam API. This downloaded every game entry on Steam as of 2/6/2024 and exported it into a csv for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import csv\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import SSLError\n",
    "\n",
    "# customisations - ensure tables show all columns\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request(url,parameters=None, steamspy=False):\n",
    "    \"\"\"Return json-formatted response of a get request using optional parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "    parameters : {'parameter': 'value'}\n",
    "        parameters to pass as part of get request\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    json_data\n",
    "        json-formatted response (dict-like)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url=url, params=parameters)\n",
    "    except SSLError as s:\n",
    "        print('SSL Error:', s)\n",
    "        \n",
    "        for i in range(5, 0, -1):\n",
    "            print('\\rWaiting... ({})'.format(i), end='')\n",
    "            time.sleep(1)\n",
    "        print('\\rRetrying.' + ' '*10)\n",
    "        \n",
    "        # recursively try again\n",
    "        return get_request(url, parameters, steamspy)\n",
    "    \n",
    "    if response:\n",
    "        try:\n",
    "            return response.json()\n",
    "        except:\n",
    "            False\n",
    "    else:\n",
    "        # We do not know how many pages steamspy has... and it seems to work well, so we will use no response to stop.\n",
    "        if steamspy:\n",
    "            return \"stop\"\n",
    "        else :\n",
    "            # response is none usually means too many requests. Wait and try again \n",
    "            print('No response, waiting 10 seconds...')\n",
    "            time.sleep(10)\n",
    "            print('Retrying.')\n",
    "            return get_request(url, parameters, steamspy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_app_data(app_list, start, stop, parser, pause):\n",
    "    \"\"\"Return list of app data generated from parser.\n",
    "    \n",
    "    parser : function to handle request\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "    \n",
    "    # iterate through each row of app_list, confined by start and stop\n",
    "    for index, appid in app_list[start:stop].items():\n",
    "        print('Current index: {}'.format(index), end='\\r')\n",
    "\n",
    "        # retrive app data for a row, handled by supplied parser, and append to list\n",
    "        try:\n",
    "            data = parser(appid)\n",
    "            app_data.append(data)\n",
    "        except:\n",
    "            print(\"Error with \"+str(appid))\n",
    "        time.sleep(pause) # prevent overloading api with requests\n",
    "    \n",
    "    return app_data\n",
    "\n",
    "\n",
    "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
    "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
    "    \"\"\"Process app data in batches, writing directly to file.\n",
    "    \n",
    "    parser : custom function to format request\n",
    "    app_list : dataframe of appid and name\n",
    "    download_path : path to store data\n",
    "    data_filename : filename to save app data\n",
    "    index_filename : filename to store highest index written\n",
    "    columns : column names for file\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    begin : starting index (get from index_filename, default 0)\n",
    "    end : index to finish (defaults to end of app_list)\n",
    "    batchsize : number of apps to write in each batch (default 100)\n",
    "    pause : time to wait after each api request (defualt 1)\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    print('Starting at index {}:\\n'.format(begin))\n",
    "    \n",
    "    # by default, process all apps in app_list\n",
    "    if end == -1:\n",
    "        end = len(app_list) + 1\n",
    "    \n",
    "    # generate array of batch begin and end points\n",
    "    batches = np.arange(begin, end, batchsize)\n",
    "    batches = np.append(batches, end)\n",
    "    \n",
    "    apps_written = 0\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in range(len(batches) - 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        start = batches[i]\n",
    "        stop = batches[i+1]\n",
    "        \n",
    "        app_data = get_app_data(app_list, start, stop, parser, pause)\n",
    "        \n",
    "        rel_path = os.path.join(download_path, data_filename)\n",
    "        \n",
    "        # writing app data to file\n",
    "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
    "            \n",
    "            for j in range(3,0,-1):\n",
    "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            writer.writerows(app_data)\n",
    "            print('\\rExported lines {}-{} to {}.'.format(start, stop-1, data_filename), end=' ')\n",
    "            \n",
    "        apps_written += len(app_data)\n",
    "        \n",
    "        idx_path = os.path.join(download_path, index_filename)\n",
    "        \n",
    "        # writing last index to file\n",
    "        with open(idx_path, 'w') as f:\n",
    "            index = stop\n",
    "            print(index, file=f)\n",
    "            \n",
    "        # logging time taken\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        batch_times.append(time_taken)\n",
    "        mean_time = statistics.mean(batch_times)\n",
    "        \n",
    "        est_remaining = (len(batches) - i - 2) * mean_time\n",
    "        \n",
    "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
    "        time_td = dt.timedelta(seconds=round(time_taken))\n",
    "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
    "        \n",
    "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
    "            \n",
    "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(download_path, index_filename):\n",
    "    \"\"\"Reset index in file to 0.\"\"\"\n",
    "    rel_path = os.path.join(download_path, index_filename)\n",
    "    \n",
    "    f= open(rel_path, 'w')\n",
    "    f.write(\"0\")\n",
    "        \n",
    "\n",
    "def get_index(download_path, index_filename):\n",
    "    \"\"\"Retrieve index from file, returning 0 if file not found.\"\"\"\n",
    "    try:\n",
    "        rel_path = os.path.join(download_path, index_filename)\n",
    "\n",
    "        with open(rel_path, 'r') as f:\n",
    "            index = int(f.readline())\n",
    "            #This just reads the initial line\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        index = 0\n",
    "        \n",
    "    return index\n",
    "\n",
    "\n",
    "def prepare_data_file(download_path, filename, index, columns):\n",
    "    \"\"\"Create file and write headers if index is 0.\"\"\"\n",
    "    if index == 0:\n",
    "        rel_path = os.path.join(download_path, filename)\n",
    "\n",
    "        with open(rel_path, 'w', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns)\n",
    "            writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Steam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppListBatch(url, parameters):\n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    steam_id = pd.DataFrame.from_dict(json_data[\"response\"][\"apps\"])\n",
    "    try:\n",
    "        more_results = json_data[\"response\"][\"have_more_results\"]\n",
    "        last_appid =  json_data[\"response\"][\"last_appid\"]\n",
    "    except:\n",
    "        more_results = False\n",
    "        last_appid = False\n",
    "    return more_results, steam_id, last_appid\n",
    "\n",
    "def get_update_ids_old(updatedlist, oldlist):\n",
    "    updatedlist['key1'] = 1\n",
    "    oldlist['key2'] = 1\n",
    "    updatedlist = pd.merge(updatedlist, oldlist, right_on=['steam_appid','name'],left_on=['appid','name'], how = 'outer')\n",
    "    updatedlist = updatedlist[~(updatedlist.key2 == updatedlist.key1)]\n",
    "    updatedlist = updatedlist.drop(['key1','key2','steam_appid'], axis=1)\n",
    "    return updatedlist\n",
    "\n",
    "def get_update_ids(idList, oldFullList):\n",
    "    #We are going to forget about names and only care about IDs.\n",
    "    idList = idList[\"appid\"]\n",
    "    oldFullList = oldFullList[\"steam_appid\"]\n",
    "    oldFullList.columns = [\"appid\"]\n",
    "    updatedList = pd.concat([idList, oldFullList])\n",
    "    updatedList = updatedList.drop_duplicates(keep=False)\n",
    "    updatedList = updatedList.reset_index(drop=True)\n",
    "    return updatedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAppList():\n",
    "    with open('../data/steam_key.txt') as f:\n",
    "        key = f.read()\n",
    "\n",
    "    url = \"https://api.steampowered.com/IStoreService/GetAppList/v1/?\"\n",
    "    parameters = {\"key\": key}\n",
    "    more_results = True\n",
    "    begin = True\n",
    "    # from the request we get the more_results flag and also the last_appid, so we use them for the next requests.\n",
    "    while (more_results):\n",
    "        more_results, steam_ids, last_appid = getAppListBatch(url, parameters)\n",
    "        parameters[\"last_appid\"] = last_appid\n",
    "        if (begin):\n",
    "            steam_allids = steam_ids\n",
    "            begin = False\n",
    "        else:\n",
    "            steam_allids = pd.concat([steam_allids, steam_ids])\n",
    "    return steam_allids\n",
    "# request 'all' from steam spy and parse into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_steam_request(appid):\n",
    "    \"\"\"Unique parser to handle data from Steam Store API.\n",
    "    \n",
    "    Returns : json formatted data (dict-like)\n",
    "    \"\"\"\n",
    "    with open('../data/steam_key.txt') as f:\n",
    "        key = f.read()\n",
    "        \n",
    "    url = \"http://store.steampowered.com/api/appdetails/\"\n",
    "    parameters = {\"appids\": appid, \"key\": key}\n",
    "    \n",
    "    json_data = get_request(url, parameters=parameters)\n",
    "    json_app_data = json_data[str(appid)]\n",
    "    \n",
    "    if json_app_data['success']:\n",
    "        data = json_app_data['data']\n",
    "    else:\n",
    "        data = {'steam_appid': appid}\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Set file parameters\n",
    "download_path = '../data/download/'\n",
    "steam_app_data = 'steam_app_data.csv'\n",
    "steam_app_data_delta = 'steam_app_data_delta.csv'\n",
    "steam_index = 'steam_index.txt'\n",
    "\n",
    "steam_columns = [\n",
    "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
    "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
    "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
    "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
    "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
    "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
    "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
    "    'background', 'content_descriptors'\n",
    "]\n",
    "\n",
    "# Overwrites last index for demonstration (would usually store highest index so can continue across sessions)\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    reset_index(download_path, steam_index)\n",
    "\n",
    "# Retrieve last index downloaded from file\n",
    "index = get_index(download_path, steam_index)\n",
    "\n",
    "# Wipe or create data file and write headers if no previous  data\n",
    "if (os.path.isfile(download_path+steam_app_data) == False):\n",
    "    prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
    "    \n",
    "# Wipe or create data file delta and write headers if index is 0\n",
    "if (os.path.isfile(download_path+steam_app_data_delta) == False):\n",
    "    prepare_data_file(download_path, steam_app_data_delta, index, steam_columns)\n",
    "    \n",
    "    \n",
    "# Here we get the list of appids from steam\n",
    "full_steam_ids = getAppList()\n",
    "\n",
    "# Here we get the real list of ids not yet in our dataframe. If this is the first time we are downloading the data, we can skip\n",
    "# This step and instead use the full app_list.\n",
    "try:\n",
    "    oldlist = pd.read_csv('../data/download/steam_app_data.csv', usecols = ['name','steam_appid'])\n",
    "    steam_ids = get_update_ids(full_steam_ids, oldlist)\n",
    "except FileNotFoundError:\n",
    "    print(\"Pre-existing file not found. First time downloading full app data from steam. This will take a while.\\n\")\n",
    "    steam_ids = full_steam_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New IDs detected: \"+str(len(steam_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I separated the long process to be able to debug it better.\n",
    "# Set end and chunksize for demonstration - remove to run through entire app list\n",
    "# Here by default we passed \"app_list\" that contained all the information and saved it, now we will modify it a bit\n",
    "# And add pre-processing and post-processing\n",
    "print(\"Adding \"+str(len(steam_ids))+\" new ids.\\n\")\n",
    "process_batches(\n",
    "    parser=parse_steam_request,\n",
    "    app_list=steam_ids,\n",
    "    download_path=download_path,\n",
    "    data_filename=steam_app_data_delta,\n",
    "    index_filename=steam_index,\n",
    "    columns=steam_columns,\n",
    "    begin=index,\n",
    "    #end=10,\n",
    "    #batchsize=5\n",
    ")\n",
    "\n",
    "try:\n",
    "    oldlist = pd.read_csv('../data/download/steam_app_data.csv')\n",
    "    # We change the old file to backup, so remove any backup named this way before...\n",
    "    os.rename('../data/download/steam_app_data.csv', '../data/download/steam_app_data_backup.csv')\n",
    "    newlist = pd.read_csv('../data/download/steam_app_data_delta.csv')\n",
    "    oldlist = oldlist.append(newlist, ignore_index=True)\n",
    "    oldlist.to_csv('../data/download/steam_app_data.csv', index=False)\n",
    "except FileNotFoundError:\n",
    "    os.rename('../data/download/steam_app_data_delta.csv', '../data/download/steam_app_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('../data/download/steam_app_data_delta.csv', '../data/download/steam_app_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Data for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data = pd.read_csv('../data/download/steam_app_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data[steam_app_data.duplicated(subset=\"steam_appid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data[steam_app_data[\"steam_appid\"] == 34330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(steam_app_data)-len(full_steam_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ids = get_update_ids(full_steam_ids, steam_app_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diff_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 135 new items added to steam since initially running this on 2/6/2024. Ran the second time on 2/8/2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting rid of dups I will only focus on the apps with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data = steam_app_data[~steam_app_data[\"name\"].isna()]\n",
    "steam_app_data = steam_app_data.drop_duplicates(subset=\"steam_appid\", keep=\"last\")\n",
    "steam_app_data.to_csv(\"../data/download/steam_app_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect downloaded data\n",
    "steam_app_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steam_app_data[steam_app_data.duplicated(subset=\"steam_appid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_steam_ids.duplicated(subset=\"appid\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_steam_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
